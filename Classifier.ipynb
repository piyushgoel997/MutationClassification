{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNzPbc+1sAVzqPTWixUwwgi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushgoel997/MutationClassification/blob/master/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOzwEXPuzGBD",
        "colab_type": "text"
      },
      "source": [
        "Import statements and declaring some global variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBWod3zZyoBH",
        "colab_type": "code",
        "outputId": "0d90680c-161f-4d15-849c-c80faa320421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import plot_model\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Conv1D, BatchNormalization, GlobalMaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "data_url = \"https://raw.githubusercontent.com/piyushgoel997/MutationClassification/master/data.csv\"\n",
        "epochs = 200\n",
        "batch_size = 8192\n",
        "\n",
        "# to stop the model from overfitting on the training data.\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20, verbose=2, mode=\"min\", restore_best_weights=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxK7n8P1zIeK",
        "colab_type": "text"
      },
      "source": [
        "Load and represent data in a one-hot encoded matrix form to be easily able to train the model.\n",
        "\n",
        "Matrix Dimentions: Number of examples, Number of Features (in this case, the length of the sequence of Amino Acids), Number of possible feature values (20 possible Amino Acids)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQHkMtSyzNG-",
        "colab_type": "code",
        "outputId": "3accc429-3ddb-4ee1-81a7-373c1294025c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "def get_aa_encoder():\n",
        "    \"\"\"\n",
        "    One-hot encoding indices for the 20 possible amino acids.\n",
        "    :return: the dictionary mapping amino acid to index in the one-hot encoding.\n",
        "    \"\"\"\n",
        "    acids = \"ARNDCEQGHILKMFPSTWYV\"\n",
        "    indices = {}\n",
        "    i = 0\n",
        "    for a in acids:\n",
        "        indices[a] = i\n",
        "        i += 1\n",
        "    return indices\n",
        "\n",
        "\n",
        "def make_matrix(seq, alt):\n",
        "    \"\"\"\n",
        "    makes a matrix where each of the 51 features correspond to the amino acid at\n",
        "    that position in the sequence after the one-hot encoding using the indices\n",
        "    provided. Also puts a -1 at in the middle most feature at the position\n",
        "    corresponding to the alt amino acid.\n",
        "    :param seq: the sequence to be encoded.\n",
        "    :param alt: the alternate amino acid\n",
        "    :return: the resultant encoded matrix (51 X 20)\n",
        "    \"\"\"\n",
        "    indices = get_aa_encoder()\n",
        "    matrix = np.zeros((len(indices), len(seq)))\n",
        "    i = 0\n",
        "    for s in seq:\n",
        "        matrix[indices[s]][i] = 1\n",
        "        i += 1\n",
        "    matrix[indices[alt]][int(len(seq) / 2)] = -1\n",
        "    return matrix.T\n",
        "\n",
        "\n",
        "def map_description(description):\n",
        "    map = {'Benign': 0, 'Benign/Likely benign': 0, 'Likely benign': 0,\n",
        "           'Likely pathogenic': 1, 'Pathogenic': 1, 'Pathogenic/Likely pathogenic': 1}\n",
        "    return map[description]\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "gene_ids = []\n",
        "mutations = pd.read_csv(data_url)\n",
        "\n",
        "for _, row in mutations.iterrows():\n",
        "    X.append(make_matrix(row[\"REF_SEQ\"], row[\"ALT_AA\"]))\n",
        "    Y.append(map_description(row[\"DESCRIPTION\"]))\n",
        "    gene_ids.append(row[\"UNIPROT_ACCESSION\"])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "X, Y = shuffle(X, Y)\n",
        "\n",
        "print(\"Input shape\", X.shape, \"\\nOutput shape\", Y.shape)\n",
        "print(\"Count of Pathogenic examples\", np.count_nonzero(Y == 1))\n",
        "print(\"Count of Benign examples\", np.count_nonzero(Y == 0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape (229502, 51, 20) \n",
            "Output shape (229502,)\n",
            "Count of Pathogenic examples 41045\n",
            "Count of Benign examples 188457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yR-p8jB54Wh",
        "colab_type": "text"
      },
      "source": [
        "Helper methods to be initialize and comple the model, and do the k-fold cross validation and provide different evaluation metrics like accuracy, loss, ROC curve, ROC AUC, PRC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtF-2ROA2Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(h=0):\n",
        "    \"\"\"\n",
        "    make, compile and return the model\n",
        "    :param h: The number of hidden layers with 64 channels to be added\n",
        "    :return: the model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=3, activation=\"relu\"))\n",
        "    for _ in range(h):\n",
        "        model.add(Conv1D(64, kernel_size=5, activation=\"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(1, kernel_size=1, activation='sigmoid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model2(h):\n",
        "    \"\"\"\n",
        "    make, compile and return the model. Incresed the kernel size.\n",
        "    :param h: The number of hidden layers with 64 channels to be added\n",
        "    :return: the model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=5, activation=\"relu\"))\n",
        "    for _ in range(h):\n",
        "        model.add(Conv1D(64, kernel_size=7, activation=\"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Conv1D(32, kernel_size=5, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(1, kernel_size=1, activation='sigmoid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def k_fold_cross_validation(h, X, Y, k=10, epochs=20, batch_size=512, callbacks=None, get_model=get_model):\n",
        "    \"\"\"\n",
        "    Make the model, train it, and validate it using k-fold cross validation.\n",
        "    :param h: The number of hidden layers with 64 channels to be added in the model.\n",
        "    :param X: input to the model.\n",
        "    :param Y: expected outputs of the model.\n",
        "    :return: average accuracy, avg auc, k roc curve coordinates, k prc curve coordinates.\n",
        "    \"\"\"\n",
        "    avg_accuracy_tain = 0\n",
        "    avg_accuracy_val = 0\n",
        "    avg_loss_train = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_roc_auc = 0\n",
        "    roc_curves = []\n",
        "    prc_curves = []\n",
        "    summary = []\n",
        "    results = \"\"\n",
        "    for i in range(k):\n",
        "        model = get_model(h)\n",
        "        print(\"========Cross Validation:\", str(i + 1) + \"/\" + str(k) + \"========\")\n",
        "        start = int(X.shape[0] * (i / k))\n",
        "        end = int(X.shape[0] * ((i + 1) / k))\n",
        "        test_data = X[start:end]\n",
        "        y_test = Y[start:end]\n",
        "        train_data = np.concatenate([X[:start], X[end:]])\n",
        "        y_train = np.concatenate([Y[:start], Y[end:]])\n",
        "        history = model.fit(train_data, y_train, validation_data=(test_data, y_test), callbacks=callbacks, batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "        test_out = model.predict(test_data)\n",
        "        if results == \"\":\n",
        "            results = test_out\n",
        "        else:\n",
        "            results = np.concatenate(results, test_out)\n",
        "        fpr, tpr, _ = roc_curve(y_test, test_out)\n",
        "        roc_curves.append((fpr, tpr))\n",
        "        avg_roc_auc += auc(fpr, tpr)\n",
        "        p, r, _ = precision_recall_curve(y_test, test_out)\n",
        "        prc_curves.append((r, p))\n",
        "        avg_accuracy_tain += history.history['accuracy'][-1]\n",
        "        avg_accuracy_val += history.history['val_accuracy'][-1]\n",
        "        avg_loss_train += history.history['loss'][-1]\n",
        "        avg_loss_val += history.history['val_loss'][-1]\n",
        "        \n",
        "        if len(summary) == 0: model.summary(print_fn=lambda x: summary.append(x))\n",
        "\n",
        "        del model\n",
        "\n",
        "    return {\"avg_accuracy_train\": avg_accuracy_tain / k, \"avg_accuracy_val\": avg_accuracy_val / k,\n",
        "            \"avg_loss_train\": avg_loss_train / k, \"avg_loss_val\": avg_loss_val / k,\n",
        "            \"avg_roc_auc\": avg_roc_auc / k, \"roc\": roc_curves, \"prc\": prc_curves,\n",
        "            \"model_summary\": \"\\n\".join(summary), \"predictions\": results}\n",
        "\n",
        "\n",
        "def genes_with_both(gene_ids, Y, pred):\n",
        "    \"\"\"\n",
        "    Gets the accuracy of the predictions on genes containing both pathogenic and benign mutations.\n",
        "    Also the number of genes conatining both, and the number of examples belonging to those genes.\n",
        "    \"\"\"\n",
        "    patho = set()\n",
        "    benign = set()\n",
        "    for i in range(len(gene_ids)):\n",
        "        if Y[i] == 1:\n",
        "            patho.add(gene_ids[i])\n",
        "        else:\n",
        "            benign.add(gene_ids[i])\n",
        "    intersec = patho.intersection(benign)\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for y, y_, id in zip(Y, pred, gene_ids):\n",
        "        if id in intersec:\n",
        "            y_true.append(y)\n",
        "            y_pred.append(y_)\n",
        "    return {\"Accuracy\": accuracy_score(y_true, y_pred), \"total\": len(y_true), \"genes_with_both\": len(intersec)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3dqV79ns9ap",
        "colab_type": "text"
      },
      "source": [
        "Training and evaluating a few different models (with different number of hidden layers) and chosing the best one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejkc1dBeEr5",
        "colab_type": "code",
        "outputId": "6a21a328-0bd7-43cf-80bd-2f1ff8c890b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# best_h_acc = -1  # best h value when chosen based on the validation accuracy.\n",
        "# best_acc = 0\n",
        "best_h_auc = -1  # best h value when chosen based on the validation auc value.\n",
        "best_auc = 0\n",
        "best_pred = \"\"\n",
        "\n",
        "over_all_start_time = time.time()\n",
        "\n",
        "for h in [1, 2, 4]:\n",
        "  for model_maker in [get_model, get_model2]:\n",
        "    start_time = time.time()\n",
        "    print(\"==========Starting for h =\", str(h) + \"==========\")\n",
        "\n",
        "    metrics = k_fold_cross_validation(h, X, Y, k=5, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping_callback], get_model=model_maker)\n",
        "\n",
        "    print(metrics[\"model_summary\"])\n",
        "\n",
        "    print(\"Final metrics for this model\")\n",
        "    print(\"Average of the final training set accuracy over all folds =\", str(metrics[\"avg_accuracy_train\"]))\n",
        "    print(\"Average of the final training set loss over all folds =\", str(metrics[\"avg_loss_train\"]))\n",
        "    print(\"Average of the final validation set accuracy over all folds =\", str(metrics[\"avg_accuracy_val\"]))\n",
        "    print(\"Average of the final validation set loss over all folds =\", str(metrics[\"avg_loss_val\"]))\n",
        "    print(\"Average of the ROC AUC over all folds =\", str(metrics[\"avg_roc_auc\"]))\n",
        "    pred = metrics[\"predictions\"]\n",
        "\n",
        "    # TODO get gene_ids in the variable\n",
        "    print(\"Genes with both pathogenic and benign mutations:\")\n",
        "    print(genes_with_both(gene_ids, Y, pred))\n",
        "\n",
        "    for c in metrics[\"roc\"]: plt.plot(*c)\n",
        "    plt.xlabel('False positive rate')\n",
        "    plt.ylabel('True positive rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "\n",
        "    for c in metrics[\"prc\"]: plt.plot(*c)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('PRC curve')\n",
        "    plt.show()\n",
        "\n",
        "    # if metrics[\"avg_accuracy_val\"] > best_acc:\n",
        "    #     best_h_acc = h\n",
        "    #     best_acc = metrics[\"avg_accuracy_val\"]\n",
        "\n",
        "    if metrics[\"avg_roc_auc\"] > best_auc:\n",
        "        best_h_auc = h\n",
        "        best_auc = metrics[\"avg_roc_auc\"]\n",
        "        best_pred = pred\n",
        "\n",
        "    print(\"Time taken for this model\", str(time.time() - start_time))\n",
        "\n",
        "    print(\"===============Finished===============\")\n",
        "\n",
        "print(\"Total time taken\", str(time.time() - over_all_start_time))\n",
        "print(\"The h value for the best validation accuracy is\", str(best_h_acc), \"and the corresponding accuracy is\",\n",
        "      str(best_acc))\n",
        "print(\"The h value for the best auc is\", str(best_h_auc), \"and the corresponding auc is\", str(best_auc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========Starting for h = 1==========\n",
            "========Cross Validation: 1/5========\n",
            "Epoch 1/200\n",
            "23/23 - 2s - loss: 1.1207 - accuracy: 0.1793 - val_loss: 0.2655 - val_accuracy: 0.1769\n",
            "Epoch 2/200\n",
            "23/23 - 1s - loss: 0.4682 - accuracy: 0.1793 - val_loss: 0.2794 - val_accuracy: 0.1769\n",
            "Epoch 3/200\n",
            "23/23 - 1s - loss: 0.3285 - accuracy: 0.1796 - val_loss: 0.2782 - val_accuracy: 0.1769\n",
            "Epoch 4/200\n",
            "23/23 - 1s - loss: 0.2465 - accuracy: 0.5431 - val_loss: 0.2567 - val_accuracy: 0.1769\n",
            "Epoch 5/200\n",
            "23/23 - 1s - loss: 0.1933 - accuracy: 0.8234 - val_loss: 0.2320 - val_accuracy: 0.8231\n",
            "Epoch 6/200\n",
            "23/23 - 1s - loss: 0.1617 - accuracy: 0.8250 - val_loss: 0.2094 - val_accuracy: 0.8231\n",
            "Epoch 7/200\n",
            "23/23 - 1s - loss: 0.1461 - accuracy: 0.8268 - val_loss: 0.1923 - val_accuracy: 0.8231\n",
            "Epoch 8/200\n",
            "23/23 - 1s - loss: 0.1386 - accuracy: 0.8340 - val_loss: 0.1802 - val_accuracy: 0.8231\n",
            "Epoch 9/200\n",
            "23/23 - 1s - loss: 0.1340 - accuracy: 0.8365 - val_loss: 0.1706 - val_accuracy: 0.8231\n",
            "Epoch 10/200\n",
            "23/23 - 1s - loss: 0.1308 - accuracy: 0.8379 - val_loss: 0.1638 - val_accuracy: 0.8231\n",
            "Epoch 11/200\n",
            "23/23 - 1s - loss: 0.1278 - accuracy: 0.8396 - val_loss: 0.1594 - val_accuracy: 0.8231\n",
            "Epoch 12/200\n",
            "23/23 - 1s - loss: 0.1244 - accuracy: 0.8424 - val_loss: 0.1566 - val_accuracy: 0.8231\n",
            "Epoch 13/200\n",
            "23/23 - 1s - loss: 0.1219 - accuracy: 0.8454 - val_loss: 0.1536 - val_accuracy: 0.8231\n",
            "Epoch 14/200\n",
            "23/23 - 1s - loss: 0.1193 - accuracy: 0.8483 - val_loss: 0.1513 - val_accuracy: 0.8231\n",
            "Epoch 15/200\n",
            "23/23 - 1s - loss: 0.1167 - accuracy: 0.8512 - val_loss: 0.1507 - val_accuracy: 0.8231\n",
            "Epoch 16/200\n",
            "23/23 - 1s - loss: 0.1140 - accuracy: 0.8528 - val_loss: 0.1488 - val_accuracy: 0.8231\n",
            "Epoch 17/200\n",
            "23/23 - 1s - loss: 0.1110 - accuracy: 0.8552 - val_loss: 0.1456 - val_accuracy: 0.8231\n",
            "Epoch 18/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ecabe18acc50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==========Starting for h =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_maker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-759a8b7d573b>\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(h, X, Y, k, epochs, batch_size, callbacks, get_model)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtest_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97gO4DUctKCR",
        "colab_type": "text"
      },
      "source": [
        "Now training the best model architecture using the whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pe2Lwo6ofTs",
        "colab_type": "code",
        "outputId": "d34aa5de-a6f8-48af-9ff6-55c9049e564f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_h = best_h_auc\n",
        "\n",
        "model = get_model(best_h)\n",
        "history = model.fit(X, Y, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "\n",
        "plot_model(model, to_file='model.png')\n",
        "model.summary()\n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 3s 89ms/step - loss: 0.7703 - accuracy: 0.1797\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.3014 - accuracy: 0.2327\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.2099 - accuracy: 0.8036\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.1691 - accuracy: 0.8278\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1505 - accuracy: 0.8301\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1416 - accuracy: 0.8344\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1392 - accuracy: 0.8369\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.1346 - accuracy: 0.8394\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.1305 - accuracy: 0.8424\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.1252 - accuracy: 0.8465\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.1236 - accuracy: 0.8494\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1169 - accuracy: 0.8543\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1199 - accuracy: 0.8540\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1114 - accuracy: 0.8598\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1061 - accuracy: 0.8652\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.1033 - accuracy: 0.8690\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0950 - accuracy: 0.8766\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0916 - accuracy: 0.8821\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0887 - accuracy: 0.8861\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0858 - accuracy: 0.8893\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0833 - accuracy: 0.8928\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0811 - accuracy: 0.8966\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0794 - accuracy: 0.8989\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0768 - accuracy: 0.9023\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0731 - accuracy: 0.9064\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0718 - accuracy: 0.9098\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0715 - accuracy: 0.9089\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0716 - accuracy: 0.9098\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0685 - accuracy: 0.9144\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0676 - accuracy: 0.9128\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0643 - accuracy: 0.9206\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0645 - accuracy: 0.9205\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0625 - accuracy: 0.9224\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0607 - accuracy: 0.9247\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0598 - accuracy: 0.9268\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0635 - accuracy: 0.9258\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.1371 - accuracy: 0.8236\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0988 - accuracy: 0.8700\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0836 - accuracy: 0.8892\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0785 - accuracy: 0.8939\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0715 - accuracy: 0.9040\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0692 - accuracy: 0.9105\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0678 - accuracy: 0.9129\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0638 - accuracy: 0.9176\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0609 - accuracy: 0.9221\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0608 - accuracy: 0.9242\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0586 - accuracy: 0.9245\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0552 - accuracy: 0.9300\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0559 - accuracy: 0.9304\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0540 - accuracy: 0.9337\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0534 - accuracy: 0.9344\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0509 - accuracy: 0.9374\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0492 - accuracy: 0.9394\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0487 - accuracy: 0.9419\n",
            "Epoch 55/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0515 - accuracy: 0.9361\n",
            "Epoch 56/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0491 - accuracy: 0.9401\n",
            "Epoch 57/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0464 - accuracy: 0.9452\n",
            "Epoch 58/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0456 - accuracy: 0.9460\n",
            "Epoch 59/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0427 - accuracy: 0.9493\n",
            "Epoch 60/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0429 - accuracy: 0.9499\n",
            "Epoch 61/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0419 - accuracy: 0.9513\n",
            "Epoch 62/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0456 - accuracy: 0.9442\n",
            "Epoch 63/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0405 - accuracy: 0.9527\n",
            "Epoch 64/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0393 - accuracy: 0.9564\n",
            "Epoch 65/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0448 - accuracy: 0.9467\n",
            "Epoch 66/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0451 - accuracy: 0.9464\n",
            "Epoch 67/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0409 - accuracy: 0.9518\n",
            "Epoch 68/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0395 - accuracy: 0.9556\n",
            "Epoch 69/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0410 - accuracy: 0.9510\n",
            "Epoch 70/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0378 - accuracy: 0.9574\n",
            "Epoch 71/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0405 - accuracy: 0.9527\n",
            "Epoch 72/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0447 - accuracy: 0.9474\n",
            "Epoch 73/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0365 - accuracy: 0.9568\n",
            "Epoch 74/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0335 - accuracy: 0.9621\n",
            "Epoch 75/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0351 - accuracy: 0.9603\n",
            "Epoch 76/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0371 - accuracy: 0.9566\n",
            "Epoch 77/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0354 - accuracy: 0.9598\n",
            "Epoch 78/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0334 - accuracy: 0.9623\n",
            "Epoch 79/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0286 - accuracy: 0.9684\n",
            "Epoch 80/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0306 - accuracy: 0.9668\n",
            "Epoch 81/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0347 - accuracy: 0.9593\n",
            "Epoch 82/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0314 - accuracy: 0.9649\n",
            "Epoch 83/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0325 - accuracy: 0.9627\n",
            "Epoch 84/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0342 - accuracy: 0.9600\n",
            "Epoch 85/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0331 - accuracy: 0.9617\n",
            "Epoch 86/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0325 - accuracy: 0.9624\n",
            "Epoch 87/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0342 - accuracy: 0.9598\n",
            "Epoch 88/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0297 - accuracy: 0.9674\n",
            "Epoch 89/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0274 - accuracy: 0.9697\n",
            "Epoch 90/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0299 - accuracy: 0.9648\n",
            "Epoch 91/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0304 - accuracy: 0.9655\n",
            "Epoch 92/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0368 - accuracy: 0.9561\n",
            "Epoch 93/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0451 - accuracy: 0.9435\n",
            "Epoch 94/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0302 - accuracy: 0.9656\n",
            "Epoch 95/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0338 - accuracy: 0.9602\n",
            "Epoch 96/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0321 - accuracy: 0.9624\n",
            "Epoch 97/100\n",
            "29/29 [==============================] - 2s 85ms/step - loss: 0.0296 - accuracy: 0.9662\n",
            "Epoch 98/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0275 - accuracy: 0.9691\n",
            "Epoch 99/100\n",
            "29/29 [==============================] - 2s 83ms/step - loss: 0.0285 - accuracy: 0.9664\n",
            "Epoch 100/100\n",
            "29/29 [==============================] - 2s 84ms/step - loss: 0.0277 - accuracy: 0.9681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUyIQEYk5JQIRkZj7/ysoG/VgGB73AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_80 (Conv1D)           multiple                  1952      \n",
            "_________________________________________________________________\n",
            "conv1d_81 (Conv1D)           multiple                  10304     \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc multiple                  256       \n",
            "_________________________________________________________________\n",
            "conv1d_82 (Conv1D)           multiple                  20544     \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc multiple                  256       \n",
            "_________________________________________________________________\n",
            "conv1d_83 (Conv1D)           multiple                  20544     \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc multiple                  256       \n",
            "_________________________________________________________________\n",
            "conv1d_84 (Conv1D)           multiple                  20544     \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc multiple                  256       \n",
            "_________________________________________________________________\n",
            "conv1d_85 (Conv1D)           multiple                  6176      \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc multiple                  128       \n",
            "_________________________________________________________________\n",
            "conv1d_86 (Conv1D)           multiple                  33        \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc multiple                  4         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_15 (Glo multiple                  0         \n",
            "=================================================================\n",
            "Total params: 81,253\n",
            "Trainable params: 80,675\n",
            "Non-trainable params: 578\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj3X5RhNbeJr",
        "colab_type": "text"
      },
      "source": [
        "Trying out models with wider hidden layers."
      ]
    }
  ]
}